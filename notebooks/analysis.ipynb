{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "sys.path.append(\"..\")  # allow importing from src\n",
    "from src.utils import (\n",
    "    load_student_files,\n",
    "    score_diagnosticity,\n",
    "    score_conceptual_focus,\n",
    "    score_solution_neutrality\n",
    ")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Paths\n",
    "RAW_RESULTS_DIR = Path(\"../results/raw\")\n",
    "PROCESSED_DIR = Path(\"../results/processed\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODELS = [\"chatgpt\", \"gemini\", \"claude\", \"perplexity\", \"starcoder\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: Load Results\n",
    "summary = []\n",
    "\n",
    "for file_path in RAW_RESULTS_DIR.glob(\"*_results.json\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    row = {\"filename\": file_path.stem.replace(\"_results\", \"\")}\n",
    "\n",
    "    for model in MODELS:\n",
    "        text = results.get(model, \"\")\n",
    "        row[f\"{model}_diagnosticity\"] = score_diagnosticity(text)\n",
    "        row[f\"{model}_conceptual_focus\"] = score_conceptual_focus(text)\n",
    "        row[f\"{model}_solution_neutrality\"] = score_solution_neutrality(text)\n",
    "\n",
    "    summary.append(row)\n",
    "\n",
    "df = pd.DataFrame(summary)\n",
    "df.to_csv(PROCESSED_DIR / \"analysis_summary.csv\", index=False)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Visualize Diagnosticity by Model\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=df[[f\"{m}_diagnosticity\" for m in MODELS]])\n",
    "plt.title(\"Diagnosticity Scores by Model\")\n",
    "plt.ylabel(\"Diagnosticity Score\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Visualize Conceptual Focus\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=df[[f\"{m}_conceptual_focus\" for m in MODELS]])\n",
    "plt.title(\"Conceptual Focus Scores by Model\")\n",
    "plt.ylabel(\"Conceptual Focus Score\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualize Solution Neutrality\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=df[[f\"{m}_solution_neutrality\" for m in MODELS]])\n",
    "plt.title(\"Solution Neutrality Scores by Model\")\n",
    "plt.ylabel(\"Solution Neutrality (1=Solution-Neutral)\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
